{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook processes ITP Level 3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: itp1final.zip\n",
      "Downloaded: itp2final.zip\n",
      "Downloaded: itp3final.zip\n",
      "Downloaded: itp4final.zip\n",
      "Downloaded: itp5final.zip\n",
      "Downloaded: itp6final.zip\n",
      "Downloaded: itp7final.zip\n",
      "Downloaded: itp8final.zip\n",
      "Downloaded: itp9final.zip\n",
      "Downloaded: itp10final.zip\n",
      "Downloaded: itp11final.zip\n",
      "Downloaded: itp12final.zip\n",
      "Downloaded: itp13final.zip\n",
      "Downloaded: itp14final.zip\n",
      "Downloaded: itp15final.zip\n",
      "Downloaded: itp16final.zip\n",
      "Downloaded: itp17final.zip\n",
      "Downloaded: itp18final.zip\n",
      "Downloaded: itp19final.zip\n",
      "Downloaded: itp21final.zip\n",
      "Downloaded: itp22final.zip\n",
      "Downloaded: itp23final.zip\n",
      "Downloaded: itp24final.zip\n",
      "Downloaded: itp25final.zip\n",
      "Downloaded: itp26final.zip\n",
      "Downloaded: itp27final.zip\n",
      "Downloaded: itp28final.zip\n",
      "Downloaded: itp29final.zip\n",
      "Downloaded: itp30final.zip\n",
      "Downloaded: itp32final.zip\n",
      "Downloaded: itp33final.zip\n",
      "Downloaded: itp34final.zip\n",
      "Downloaded: itp35final.zip\n",
      "Downloaded: itp36final.zip\n",
      "Downloaded: itp37final.zip\n",
      "Downloaded: itp38final.zip\n",
      "Downloaded: itp39_1final.zip\n",
      "Downloaded: itp41final.zip\n",
      "Downloaded: itp42final.zip\n",
      "Downloaded: itp43final.zip\n",
      "Downloaded: itp47final.zip\n",
      "Downloaded: itp48final.zip\n",
      "Downloaded: itp49final.zip\n",
      "Downloaded: itp51final.zip\n",
      "Downloaded: itp52final.zip\n",
      "Downloaded: itp53final.zip\n",
      "Downloaded: itp54final.zip\n",
      "Downloaded: itp55final.zip\n",
      "Downloaded: itp56final.zip\n",
      "Downloaded: itp57final.zip\n",
      "Downloaded: itp58final.zip\n",
      "Downloaded: itp59final.zip\n",
      "Downloaded: itp60final.zip\n",
      "Downloaded: itp61final.zip\n",
      "Downloaded: itp62final.zip\n",
      "Downloaded: itp63final.zip\n",
      "Downloaded: itp64final.zip\n",
      "Downloaded: itp65final.zip\n",
      "Downloaded: itp66final.zip\n",
      "Downloaded: itp68final.zip\n",
      "Downloaded: itp69final.zip\n",
      "Downloaded: itp70final.zip\n",
      "Downloaded: itp71final.zip\n",
      "Downloaded: itp72final.zip\n",
      "Downloaded: itp73final.zip\n",
      "Downloaded: itp74final.zip\n",
      "Downloaded: itp75final.zip\n",
      "Downloaded: itp76final.zip\n",
      "Downloaded: itp77final.zip\n",
      "Downloaded: itp78final.zip\n",
      "Downloaded: itp79final.zip\n",
      "Downloaded: itp80final.zip\n",
      "Downloaded: itp81final.zip\n",
      "Downloaded: itp82final.zip\n",
      "Downloaded: itp83final.zip\n",
      "Downloaded: itp84final.zip\n",
      "Downloaded: itp85final.zip\n",
      "Downloaded: itp86final.zip\n",
      "Downloaded: itp87final.zip\n",
      "Downloaded: itp88final.zip\n",
      "Downloaded: itp89final.zip\n",
      "Downloaded: itp90final.zip\n",
      "Downloaded: itp91final.zip\n",
      "Downloaded: itp92final.zip\n",
      "Downloaded: itp94final.zip\n",
      "Downloaded: itp95final.zip\n",
      "Downloaded: itp97final.zip\n",
      "Downloaded: itp98final.zip\n",
      "Downloaded: itp99final.zip\n",
      "Downloaded: itp100final.zip\n",
      "Downloaded: itp101final.zip\n",
      "Downloaded: itp102final.zip\n",
      "Downloaded: itp103final.zip\n",
      "Downloaded: itp104final.zip\n",
      "Downloaded: itp105final.zip\n",
      "Downloaded: itp107final.zip\n",
      "Downloaded: itp108final.zip\n",
      "Downloaded: itp109final.zip\n",
      "Downloaded: itp110final.zip\n",
      "Downloaded: itp111final.zip\n",
      "Downloaded: itp113final.zip\n",
      "Downloaded: itp114final.zip\n",
      "Downloaded: itp115final.zip\n",
      "Downloaded: itp116final.zip\n",
      "Downloaded: itp117final.zip\n",
      "Downloaded: itp118final.zip\n",
      "Downloaded: itp119final.zip\n",
      "Downloaded: itp120final.zip\n",
      "Downloaded: itp121final.zip\n",
      "Downloaded: itp122final.zip\n",
      "Downloaded: itp123final.zip\n",
      "Downloaded: itp125final.zip\n",
      "Downloaded: itp126final.zip\n",
      "Downloaded: itp127final.zip\n",
      "Downloaded: itp128final.zip\n",
      "Downloaded: itp129final.zip\n",
      "Downloaded: itp130final.zip\n",
      "Downloaded: itp131final.zip\n",
      "All files downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL of the HTML page\n",
    "url = \"https://scienceweb.whoi.edu/itp/data/\"\n",
    "\n",
    "def download_zip(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the subdirectories\n",
    "    subdirectories = [a['href'] for a in soup.find_all('a', href=lambda href: href and href.endswith('/') and href not in ['../', './'])]\n",
    "\n",
    "    # Choose the base directory path to save the downloaded files\n",
    "    base_directory = '/Users/ko389/Documents/Arctic_Water_Masses/Arctic_data/T_S_data_processing/data/itp_Arctic'\n",
    "\n",
    "    # Create the base directory if it doesn't exist\n",
    "    os.makedirs(base_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate through each subdirectory\n",
    "    for subdirectory in subdirectories:\n",
    "        subdirectory_url = urljoin(url, subdirectory)\n",
    "\n",
    "        # Send a GET request to the subdirectory URL\n",
    "        subdirectory_response = requests.get(subdirectory_url)\n",
    "\n",
    "        # Parse the HTML content of the subdirectory\n",
    "        subdirectory_soup = BeautifulSoup(subdirectory_response.text, 'html.parser')\n",
    "\n",
    "        # Find all the <a> tags with href ending in 'final.zip' in the subdirectory\n",
    "        links = subdirectory_soup.find_all('a', href=lambda href: href and href.endswith('final.zip'))\n",
    "\n",
    "        # Download each file in the subdirectory\n",
    "        for link in links:\n",
    "            # Construct the absolute URL for the file\n",
    "            file_url = urljoin(subdirectory_url, link['href'])\n",
    "            file_name = link['href']\n",
    "            file_path = os.path.join(base_directory, file_name)\n",
    "\n",
    "            # Ensure the subdirectory structure is created\n",
    "            subdirectory_path = os.path.join(base_directory, subdirectory)\n",
    "            os.makedirs(subdirectory_path, exist_ok=True)\n",
    "\n",
    "            # Send a GET request to download the file\n",
    "            file_response = requests.get(file_url)\n",
    "\n",
    "            # Save the file to disk\n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(file_response.content)\n",
    "\n",
    "            print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "    print(\"All files downloaded successfully.\")\n",
    "\n",
    "# Call the function with the provided URL\n",
    "download_zip(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded itpsys79 to /Users/ko389/Documents/Arctic_Water_Masses/Arctic_data/T_S_data_processing/data/itp_Arctic\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "directory_path = '/Users/ko389/Documents/Arctic_Water_Masses/Arctic_data/T_S_data_processing/data/itp_Arctic'\n",
    "\n",
    "# Iterate over each .zip file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.zip'):\n",
    "        zip_file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "        # Extract the contents of the .zip file\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            # Iterate over each file in the .zip file\n",
    "            for inner_file_name in zip_ref.namelist():\n",
    "                # Extract the file\n",
    "                extracted_file = zip_ref.extract(inner_file_name, directory_path)\n",
    "        # Delete the file\n",
    "        os.remove(zip_file_path)\n",
    "\n",
    "print(f\"Downloaded {file_name} to {directory_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_itp_and_profile_number(filename):\n",
    "    match = re.search(r'itp(\\d+)grd(\\d+)', filename)\n",
    "    if match:\n",
    "        itp_number, profile_number = match.groups()\n",
    "        return int(itp_number), int(profile_number)\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "directory = '/Users/ko389/Documents/Arctic_Water_Masses/Arctic_data/T_S_data_processing/data/itp_Arctic'\n",
    "dfs = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.dat') and ('sami' not in filename.lower() and 'micro' not in filename.lower()):\n",
    "        try:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as dat_file:\n",
    "                lines = dat_file.readlines()\n",
    "                # Remove the '%endofdat' at end of profiles\n",
    "                lines = [line for line in lines if line != '%endofdat\\n']\n",
    "                # define headers\n",
    "                header_1 = re.sub(r'%ITP \\d+, profile \\d+:', '', lines[0]).split()\n",
    "                header_2 = lines[2].split()\n",
    "                # get variable data\n",
    "                itp_number, profile_number = get_itp_and_profile_number(filename)\n",
    "                year, day, longitude, latitude, max_depth = lines[1].split()\n",
    "                data = [line.strip() for line in lines[3:] if line.strip()]\n",
    "                split_data = [line.split() for line in data]\n",
    "                # add nans to itp profiles that don't have dissolved oxygen\n",
    "                if len(header_2) == 4:\n",
    "                    for sublist in split_data:\n",
    "                        sublist.insert(3, 'NaN')\n",
    "                # remove velocity data from itp profiles that have MAVS and no dissolved oxygen\n",
    "                if len(header_2) == 8:\n",
    "                    for sublist in split_data:\n",
    "                        sublist[:] = sublist[:-4]\n",
    "                        sublist.insert(3, 'NaN')\n",
    "                # remove velocity data from itp profiles that have MAVS and dissolved oxygen\n",
    "                if len(header_2) == 9:\n",
    "                    for sublist in split_data:\n",
    "                        sublist[:] = sublist[:-4]\n",
    "\n",
    "                columns = ['pressure', 'temperature', 'salinity', 'dissolved_oxygen', 'nobs']\n",
    "                df = pd.DataFrame(split_data, columns=columns)\n",
    "                df['year'] = year\n",
    "                df['day'] = day\n",
    "                df['longitude'] = longitude\n",
    "                df['latitude'] = latitude\n",
    "                df['itp_no'] = itp_number\n",
    "                df['prof_no'] = profile_number\n",
    "\n",
    "                dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {str(e)}\")\n",
    "\n",
    "# Now you can check the printed error messages to identify which file is causing the problem. \n",
    "# Hopefully no files are causing the problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .dat files\n",
    "import os\n",
    "directory_path = \"/Users/ko389/Documents/Arctic_Water_Masses/Arctic_data/T_S_data_processing/data/itp_Arctic\"\n",
    "\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.dat'):\n",
    "        dat_file_path = os.path.join(directory_path, file_name)\n",
    "        os.remove(dat_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>salinity</th>\n",
       "      <th>dissolved_oxygen</th>\n",
       "      <th>nobs</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>itp_no</th>\n",
       "      <th>prof_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1</td>\n",
       "      <td>-1.5021</td>\n",
       "      <td>27.5793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.34653</td>\n",
       "      <td>-148.8983</td>\n",
       "      <td>74.9978</td>\n",
       "      <td>114</td>\n",
       "      <td>1769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.9</td>\n",
       "      <td>-1.5021</td>\n",
       "      <td>27.5794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.34653</td>\n",
       "      <td>-148.8983</td>\n",
       "      <td>74.9978</td>\n",
       "      <td>114</td>\n",
       "      <td>1769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.5021</td>\n",
       "      <td>27.5794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.34653</td>\n",
       "      <td>-148.8983</td>\n",
       "      <td>74.9978</td>\n",
       "      <td>114</td>\n",
       "      <td>1769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.8</td>\n",
       "      <td>-1.5020</td>\n",
       "      <td>27.5794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.34653</td>\n",
       "      <td>-148.8983</td>\n",
       "      <td>74.9978</td>\n",
       "      <td>114</td>\n",
       "      <td>1769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.8</td>\n",
       "      <td>-1.6201</td>\n",
       "      <td>29.6559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>2013</td>\n",
       "      <td>120.75003</td>\n",
       "      <td>-117.6934</td>\n",
       "      <td>89.4396</td>\n",
       "      <td>61</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>251.0</td>\n",
       "      <td>-1.0485</td>\n",
       "      <td>33.9098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>186.25141</td>\n",
       "      <td>-154.3825</td>\n",
       "      <td>77.0370</td>\n",
       "      <td>70</td>\n",
       "      <td>3592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>252.1</td>\n",
       "      <td>-1.0369</td>\n",
       "      <td>33.9262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>186.25141</td>\n",
       "      <td>-154.3825</td>\n",
       "      <td>77.0370</td>\n",
       "      <td>70</td>\n",
       "      <td>3592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>252.5</td>\n",
       "      <td>-1.0375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>186.25141</td>\n",
       "      <td>-154.3825</td>\n",
       "      <td>77.0370</td>\n",
       "      <td>70</td>\n",
       "      <td>3592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.7</td>\n",
       "      <td>-1.1376</td>\n",
       "      <td>29.0884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1443</td>\n",
       "      <td>2019</td>\n",
       "      <td>190.81432</td>\n",
       "      <td>-129.8973</td>\n",
       "      <td>76.8103</td>\n",
       "      <td>103</td>\n",
       "      <td>4761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.4827</td>\n",
       "      <td>27.5163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280</td>\n",
       "      <td>2019</td>\n",
       "      <td>330.86611</td>\n",
       "      <td>-141.2242</td>\n",
       "      <td>75.8770</td>\n",
       "      <td>113</td>\n",
       "      <td>1273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54081938 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pressure temperature salinity dissolved_oxygen  nobs  year        day  \\\n",
       "0        6.1     -1.5021  27.5793              NaN     8  2020    7.34653   \n",
       "1        6.9     -1.5021  27.5794              NaN     8  2020    7.34653   \n",
       "2        8.0     -1.5021  27.5794              NaN     9  2020    7.34653   \n",
       "3        8.8     -1.5020  27.5794              NaN     9  2020    7.34653   \n",
       "0        5.8     -1.6201  29.6559              NaN    16  2013  120.75003   \n",
       "..       ...         ...      ...              ...   ...   ...        ...   \n",
       "244    251.0     -1.0485  33.9098              NaN     5  2014  186.25141   \n",
       "245    252.1     -1.0369  33.9262              NaN    13  2014  186.25141   \n",
       "246    252.5     -1.0375      NaN              NaN     2  2014  186.25141   \n",
       "0        6.7     -1.1376  29.0884              NaN  1443  2019  190.81432   \n",
       "0        6.0     -1.4827  27.5163              NaN   280  2019  330.86611   \n",
       "\n",
       "     longitude latitude  itp_no  prof_no  \n",
       "0    -148.8983  74.9978     114     1769  \n",
       "1    -148.8983  74.9978     114     1769  \n",
       "2    -148.8983  74.9978     114     1769  \n",
       "3    -148.8983  74.9978     114     1769  \n",
       "0    -117.6934  89.4396      61       80  \n",
       "..         ...      ...     ...      ...  \n",
       "244  -154.3825  77.0370      70     3592  \n",
       "245  -154.3825  77.0370      70     3592  \n",
       "246  -154.3825  77.0370      70     3592  \n",
       "0    -129.8973  76.8103     103     4761  \n",
       "0    -141.2242  75.8770     113     1273  \n",
       "\n",
       "[54081938 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itp= pd.concat(dfs)\n",
    "itp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "itp.sort_values(by='itp_no', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "itp = itp.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datetime from year and day of year\n",
    "itp['datetime'] = pd.to_datetime(itp['year'].astype(str) + ' ' + itp['day'].astype(int).astype(str), format='%Y %j')\n",
    "itp.drop(columns=['year', 'day'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has to have both T and S\n",
    "itp.dropna(subset=['temperature', 'salinity'], inplace=True)\n",
    "\n",
    "# Add a column for depth\n",
    "import gsw as gsw\n",
    "itp['depth'] = -gsw.z_from_p(itp['pressure'].values, itp['latitude'].values)\n",
    "\n",
    "# Create a unique 'nprof' column based on 'itp_no' and 'prof_no'\n",
    "itp['nprof'] = pd.factorize(itp['itp_no'].astype(str) + '_' + itp['prof_no'].astype(str))[0] + 1\n",
    "itp.drop(['itp_no', 'prof_no', 'nobs'], axis=1, inplace=True)\n",
    "\n",
    "# Rename temperature to potential temperature\n",
    "itp.rename(columns={'temperature':'potential_temperature', 'salinity':'practical_salinity'},inplace=True)\n",
    "\n",
    "# Name source\n",
    "itp['source']='itp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as NetCDF file successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "import xarray as xr\n",
    "\n",
    "ds = itp.to_xarray()\n",
    "\n",
    "# Define the output file path and name for the NetCDF file\n",
    "output_file_path = \"/Users/ko389/Documents/Arctic_Water_Masses/Arctic_data/T_S_data_processing/data/itp_Arctic/\"\n",
    "output_file_name = \"itp_processed.nc\"\n",
    "\n",
    "# Save the xarray Dataset as a NetCDF file\n",
    "ds.to_netcdf(output_file_path + output_file_name)\n",
    "\n",
    "print(\"DataFrame saved as NetCDF file successfully.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .csv\n",
    "\n",
    "itp.to_csv(\"/Users/ko389/Documents/Arctic_Water_Masses/Arctic_data/T_S_data_processing/data/itp_processed.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydro_env_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
